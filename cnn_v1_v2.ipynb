{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data\n",
    "execute only if u need to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Das Notebook basiert auf diesem Turoial:\n",
    "https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cv2     # for capturing videos\n",
    "import math   # for mathematical operations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from keras.preprocessing import image   # for preprocessing the images\n",
    "import numpy as np    # for mathematical operations\n",
    "from keras.utils import np_utils\n",
    "#from skimage.transform import resize   # for resizing images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from glob import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NameOfFile</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video_1.mp4</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video_2.mp4</td>\n",
       "      <td>upwards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video_3.mp4</td>\n",
       "      <td>front</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video_4.mp4</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Video_5.mp4</td>\n",
       "      <td>downwards</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    NameOfFile     labels\n",
       "0  Video_1.mp4       side\n",
       "1  Video_2.mp4    upwards\n",
       "2  Video_3.mp4      front\n",
       "3  Video_4.mp4       side\n",
       "4  Video_5.mp4  downwards"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing training labels\n",
    "labels_df = pd.read_csv('material/labels.csv', sep=';')\n",
    "\n",
    "# deleting videos with changing perspectives\n",
    "labels_df = labels_df.drop(labels_df.loc[labels_df['Wechsel'] == True].index)\n",
    "labels_df = labels_df.reset_index()\n",
    "\n",
    "# droping unused columns:\n",
    "labels_df.drop(columns=['index','Wechsel','ID'], inplace=True)\n",
    "labels_df.rename(columns={'Camera Position (Side/Front/Upwards/Downwards)':'labels'}, inplace=True)\n",
    "\n",
    "print(labels_df.shape)\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:43<00:00,  2.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# storing the frames from training videos\n",
    "for i in tqdm(range(labels_df.shape[0])):\n",
    "    count = 0\n",
    "    videoFile = labels_df['NameOfFile'][i]\n",
    "    cap = cv2.VideoCapture('material/raw_training_videos/'+videoFile)   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    #print(frameRate)\n",
    "    x=1\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        #print(frameId)\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        #if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames in a new folder named train_frames\n",
    "        else:\n",
    "            filename ='material/train_frames/' + videoFile +\"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8300/8300 [00:01<00:00, 5651.36it/s]\n"
     ]
    }
   ],
   "source": [
    "# getting the names of all the images\n",
    "images = glob(\"material/train_frames/*.jpg\")\n",
    "train_image = []\n",
    "train_class = []\n",
    "for i in tqdm(range(len(images))):\n",
    "    # creating the image name\n",
    "    name = images[i].split('/')[2].split('_f')[0]\n",
    "    #print(name)\n",
    "    train_image.append(images[i].split('/')[2])\n",
    "    # creating the class of image\n",
    "    train_class.append(labels_df['labels'].loc[labels_df['NameOfFile'] == name].values[0])\n",
    "    \n",
    "# storing the images and their class in a dataframe\n",
    "train_data = pd.DataFrame()\n",
    "train_data['image'] = train_image\n",
    "train_data['class'] = train_class\n",
    "\n",
    "# converting the dataframe into csv file \n",
    "train_data.to_csv('material/train_frames.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Video_92.mp4_frame60.jpg</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Video_68.mp4_frame6.jpg</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Video_18.mp4_frame9.jpg</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Video_16.mp4_frame54.jpg</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Video_39.mp4_frame76.jpg</td>\n",
       "      <td>side</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image class\n",
       "0  Video_92.mp4_frame60.jpg  side\n",
       "1   Video_68.mp4_frame6.jpg  side\n",
       "2   Video_18.mp4_frame9.jpg  side\n",
       "3  Video_16.mp4_frame54.jpg  side\n",
       "4  Video_39.mp4_frame76.jpg  side"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in csv with labels for frames\n",
    "train = pd.read_csv('material/train_frames.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8300/8300 [00:43<00:00, 192.24it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8300, 480, 640, 3)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in all the frames and saving them to a numpy array\n",
    "\n",
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the image size (480, 640, 3)\n",
    "    '''\n",
    "    TO DO:\n",
    "    finde heraus was die ideale input größe der bilder ist. \n",
    "    '''\n",
    "    img = image.load_img('material/train_frames/'+train['image'][i], target_size=(480, 640, 3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    '''\n",
    "    TO DO:\n",
    "    finde heraus was die optimale normalisierung der Bild daten ist\n",
    "    '''\n",
    "    img = img/480\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test set\n",
    "\n",
    "# separating the target\n",
    "y = train['class']\n",
    "\n",
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>downwards</th>\n",
       "      <th>front</th>\n",
       "      <th>side</th>\n",
       "      <th>upwards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3222</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6280</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3089</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2868</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3109</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7873</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7304</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6640 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      downwards  front  side  upwards\n",
       "3222          0      1     0        0\n",
       "6280          0      0     1        0\n",
       "722           0      1     0        0\n",
       "3089          1      0     0        0\n",
       "2868          1      0     0        0\n",
       "...         ...    ...   ...      ...\n",
       "3109          0      0     1        0\n",
       "384           0      0     1        0\n",
       "1981          0      0     1        0\n",
       "7873          0      0     1        0\n",
       "7304          1      0     0        0\n",
       "\n",
       "[6640 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the base model of pre-trained VGG16 model\n",
    "'''\n",
    "TO DO: \n",
    "finde heraus wie ein Model performt, welches von \"scratch\" creiert wird\n",
    "'''\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6640, 15, 20, 512)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for training frames\n",
    "X_train = base_model.predict(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1660, 15, 20, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extracting features for validation frames\n",
    "X_test = base_model.predict(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the training as well as validation frames in single dimension\n",
    "X_train = X_train.reshape(6640, 15*20*512)\n",
    "X_test = X_test.reshape(1660, 15*20*512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing the pixel values\n",
    "max = X_train.max()\n",
    "X_train = X_train/max\n",
    "X_test = X_test/max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6640, 153600)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of images\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(153600,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight_v2.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model wird hier trainiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "52/52 [==============================] - 4s 85ms/step - loss: 1.1756 - accuracy: 0.5491 - val_loss: 0.4119 - val_accuracy: 0.9163\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 17s 325ms/step - loss: 0.2429 - accuracy: 0.9072 - val_loss: 0.0716 - val_accuracy: 0.9795\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 17s 333ms/step - loss: 0.0774 - accuracy: 0.9739 - val_loss: 0.0107 - val_accuracy: 1.0000\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 17s 328ms/step - loss: 0.0308 - accuracy: 0.9914 - val_loss: 0.0062 - val_accuracy: 1.0000\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 17s 334ms/step - loss: 0.0276 - accuracy: 0.9919 - val_loss: 9.8323e-04 - val_accuracy: 1.0000\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 2s 48ms/step - loss: 0.0315 - accuracy: 0.9901 - val_loss: 0.0067 - val_accuracy: 0.9988\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 2s 47ms/step - loss: 0.0636 - accuracy: 0.9800 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 17s 327ms/step - loss: 0.0303 - accuracy: 0.9901 - val_loss: 1.1822e-04 - val_accuracy: 1.0000\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 2s 45ms/step - loss: 0.0243 - accuracy: 0.9925 - val_loss: 1.2009e-04 - val_accuracy: 1.0000\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 17s 329ms/step - loss: 0.0137 - accuracy: 0.9955 - val_loss: 3.3555e-05 - val_accuracy: 1.0000\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0485 - accuracy: 0.9846 - val_loss: 1.0559e-04 - val_accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 17s 329ms/step - loss: 0.0263 - accuracy: 0.9895 - val_loss: 1.4002e-05 - val_accuracy: 1.0000\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 2s 47ms/step - loss: 0.0202 - accuracy: 0.9934 - val_loss: 6.5088e-05 - val_accuracy: 1.0000\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0369 - accuracy: 0.9884 - val_loss: 1.0024e-04 - val_accuracy: 1.0000\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 2s 45ms/step - loss: 0.0299 - accuracy: 0.9889 - val_loss: 2.2760e-05 - val_accuracy: 1.0000\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0474 - accuracy: 0.9846 - val_loss: 4.3434e-04 - val_accuracy: 1.0000\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0514 - accuracy: 0.9825 - val_loss: 4.4970e-05 - val_accuracy: 1.0000\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0310 - accuracy: 0.9890 - val_loss: 8.4095e-04 - val_accuracy: 1.0000\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0400 - accuracy: 0.9848 - val_loss: 5.9441e-04 - val_accuracy: 1.0000\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 17s 327ms/step - loss: 0.0318 - accuracy: 0.9872 - val_loss: 4.0574e-06 - val_accuracy: 1.0000\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 2s 47ms/step - loss: 0.0405 - accuracy: 0.9842 - val_loss: 1.1445e-04 - val_accuracy: 1.0000\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 2s 45ms/step - loss: 0.0448 - accuracy: 0.9804 - val_loss: 3.1044e-04 - val_accuracy: 1.0000\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0415 - accuracy: 0.9825 - val_loss: 1.9873e-04 - val_accuracy: 1.0000\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 2s 46ms/step - loss: 0.0497 - accuracy: 0.9786 - val_loss: 8.5744e-06 - val_accuracy: 1.0000\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - 17s 336ms/step - loss: 0.0361 - accuracy: 0.9833 - val_loss: 3.6899e-06 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e702e5460>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model \n",
    "'''\n",
    "TO DO: optimale epochen zahl herausfinden. Führen 200 epochen zu overfitting?\n",
    "'''\n",
    "model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our Video Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model architecture and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from scipy import stats as s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base pretrained model\n",
    "base_model = VGG16(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the model architecture\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='relu', input_shape=(153600,)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained weights\n",
    "model.load_weights(\"weight_v2.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_8Vy3dlHg2w_00118.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>_8Vy3dlHg2w_00126.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>_8Vy3dlHg2w_00115.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_8Vy3dlHg2w_00108.mp4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_8Vy3dlHg2w_00110.mp4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              video_name\n",
       "0  _8Vy3dlHg2w_00118.mp4\n",
       "1  _8Vy3dlHg2w_00126.mp4\n",
       "2  _8Vy3dlHg2w_00115.mp4\n",
       "3  _8Vy3dlHg2w_00108.mp4\n",
       "4  _8Vy3dlHg2w_00110.mp4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating data Frame with test video names\n",
    "videos = glob(\"material/raw_test_videos\"+\"/*mp4\")\n",
    "\n",
    "videoNames = []\n",
    "for i in range(len(videos)):\n",
    "    videoNames.append(videos[i].split('/')[2])\n",
    "\n",
    "test = pd.DataFrame()\n",
    "test['video_name'] = videoNames\n",
    "test = test[:-1]\n",
    "test_videos = test['video_name']\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the tags\n",
    "train = pd.read_csv('material/test_labels.csv', sep=';')\n",
    "y = train['label']\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_videos.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions for test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/23 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-856b3905037e>:45: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:34<00:00,  4.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# creating two lists to store predicted and actual tags\n",
    "predict = []\n",
    "actual = []\n",
    "\n",
    "# for loop to extract frames from each test video\n",
    "for i in tqdm(range(test_videos.shape[0])):\n",
    "    count = 0\n",
    "    videoFile = test_videos[i]\n",
    "    cap = cv2.VideoCapture('material/raw_test_videos/'+videoFile)   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    x=1\n",
    "    # removing all other files from the temp folder\n",
    "    files = glob('temp/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        #if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames of this particular video in temp folder\n",
    "        else:\n",
    "            filename ='temp/' + \"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(\"temp/*.jpg\")\n",
    "    \n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(480, 640, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/480\n",
    "        prediction_images.append(img)\n",
    "    \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "    # extracting features using pre-trained model\n",
    "    prediction_images = base_model.predict(prediction_images)\n",
    "    # converting features in one dimensional array\n",
    "    prediction_images = prediction_images.reshape(prediction_images.shape[0], 15*20*512)\n",
    "    # predicting tags for each array\n",
    "    prediction = model.predict_classes(prediction_images)\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(y.columns.values[s.mode(prediction)[0][0]])\n",
    "    # appending the actual tag of the video\n",
    "    actual.append(train['label'].loc[train['NameOfFile'] == videoFile].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# checking the accuracy of the predicted tags\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['side',\n",
       " 'front',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['side',\n",
       " 'front',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
