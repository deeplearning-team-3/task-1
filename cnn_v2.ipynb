{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Das Notebook basiert auf diesem Turoial:\n",
    "# https://www.analyticsvidhya.com/blog/2019/09/step-by-step-deep-learning-tutorial-video-classification-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in csv with labels for frames\n",
    "train = pd.read_csv('material/train_frames.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8300/8300 [00:42<00:00, 193.60it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8300, 480, 640, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading in all the frames and saving them to a numpy array\n",
    "\n",
    "# creating an empty list\n",
    "train_image = []\n",
    "\n",
    "# for loop to read and store frames\n",
    "for i in tqdm(range(train.shape[0])):\n",
    "    # loading the image and keeping the image size (480, 640, 3)\n",
    "    '''\n",
    "    TO DO:\n",
    "    finde heraus was die ideale input größe der bilder ist. \n",
    "    '''\n",
    "    img = image.load_img('material/train_frames/'+train['image'][i], target_size=(480, 640, 3))\n",
    "    # converting it to array\n",
    "    img = image.img_to_array(img)\n",
    "    # normalizing the pixel value\n",
    "    '''\n",
    "    TO DO:\n",
    "    finde heraus was die optimale normalisierung der Bild daten ist\n",
    "    '''\n",
    "    img = img/480\n",
    "    # appending the image to the train_image list\n",
    "    train_image.append(img)\n",
    "    \n",
    "# converting the list to numpy array\n",
    "X = np.array(train_image)\n",
    "\n",
    "# shape of the array\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating train and test set\n",
    "\n",
    "# separating the target\n",
    "y = train['class']\n",
    "\n",
    "# creating the training and validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dummies of target variable for train and validation set\n",
    "y_train = pd.get_dummies(y_train)\n",
    "y_test = pd.get_dummies(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the architecture of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without transferlearning based on:\n",
    "# https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "init=\"he_normal\"\n",
    "inputShape = (480, 640, 3)\n",
    "reg = l2(0.0005)\n",
    "chanDim = -1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (7, 7), strides=(2, 2), padding=\"valid\",\n",
    "                 kernel_initializer=init, kernel_regularizer=reg,input_shape=inputShape))\n",
    "\n",
    "# here we stack two CONV layers on top of each other where\n",
    "# each layerswill learn a total of 32 (3x3) filters\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# stack two more CONV layers, keeping the size of each filter\n",
    "# as 3x3 but increasing to 64 total learned filters\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "# increase the number of filters again, this time to 128\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully-connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=init))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "# softmax classifier\n",
    "model.add(Dense(4))\n",
    "model.add(Activation(\"softmax\"))\n",
    "# return the constructed network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 237, 317, 16)      2368      \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 237, 317, 32)      4640      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 237, 317, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 237, 317, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 119, 159, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 119, 159, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 119, 159, 32)      128       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 119, 159, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 119, 159, 64)      18496     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 119, 159, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 119, 159, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 60, 80, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 60, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 60, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 60, 80, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 60, 80, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 60, 80, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 30, 40, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 30, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 153600)            0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               78643712  \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 2052      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 78,942,724\n",
      "Trainable params: 78,940,804\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function to save the weights of best model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "mcp_save = ModelCheckpoint('weight_v2.1.hdf5', save_best_only=True, monitor='val_loss', mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model wird hier trainiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "52/52 [==============================] - 41s 788ms/step - loss: 0.6062 - accuracy: 0.9658 - val_loss: 7.5193 - val_accuracy: 0.2199\n",
      "Epoch 2/25\n",
      "52/52 [==============================] - 42s 809ms/step - loss: 0.4500 - accuracy: 0.9998 - val_loss: 5.9238 - val_accuracy: 0.4807\n",
      "Epoch 3/25\n",
      "52/52 [==============================] - 28s 540ms/step - loss: 0.4311 - accuracy: 1.0000 - val_loss: 6.5491 - val_accuracy: 0.4416\n",
      "Epoch 4/25\n",
      "52/52 [==============================] - 41s 793ms/step - loss: 0.4102 - accuracy: 1.0000 - val_loss: 3.2362 - val_accuracy: 0.4970\n",
      "Epoch 5/25\n",
      "52/52 [==============================] - 54s 1s/step - loss: 0.3886 - accuracy: 0.9998 - val_loss: 1.1418 - val_accuracy: 0.7861\n",
      "Epoch 6/25\n",
      "52/52 [==============================] - 35s 665ms/step - loss: 0.3733 - accuracy: 0.9983 - val_loss: 0.5474 - val_accuracy: 0.9325\n",
      "Epoch 7/25\n",
      "52/52 [==============================] - 26s 491ms/step - loss: 0.3532 - accuracy: 0.9986 - val_loss: 0.9230 - val_accuracy: 0.8367\n",
      "Epoch 8/25\n",
      "52/52 [==============================] - 34s 645ms/step - loss: 0.3256 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.9753\n",
      "Epoch 9/25\n",
      "52/52 [==============================] - 34s 646ms/step - loss: 0.3047 - accuracy: 1.0000 - val_loss: 0.3009 - val_accuracy: 0.9982\n",
      "Epoch 10/25\n",
      "52/52 [==============================] - 34s 645ms/step - loss: 0.2848 - accuracy: 1.0000 - val_loss: 0.2757 - val_accuracy: 0.9994\n",
      "Epoch 11/25\n",
      "52/52 [==============================] - 36s 685ms/step - loss: 0.2657 - accuracy: 1.0000 - val_loss: 0.2565 - val_accuracy: 1.0000\n",
      "Epoch 12/25\n",
      "52/52 [==============================] - 35s 678ms/step - loss: 0.2477 - accuracy: 1.0000 - val_loss: 0.2410 - val_accuracy: 0.9994\n",
      "Epoch 13/25\n",
      "52/52 [==============================] - 35s 675ms/step - loss: 0.2306 - accuracy: 1.0000 - val_loss: 0.2240 - val_accuracy: 0.9994\n",
      "Epoch 14/25\n",
      "52/52 [==============================] - 33s 639ms/step - loss: 0.2147 - accuracy: 1.0000 - val_loss: 0.2086 - val_accuracy: 0.9994\n",
      "Epoch 15/25\n",
      "52/52 [==============================] - 33s 644ms/step - loss: 0.1998 - accuracy: 1.0000 - val_loss: 0.1954 - val_accuracy: 0.9994\n",
      "Epoch 16/25\n",
      "52/52 [==============================] - 34s 647ms/step - loss: 0.1859 - accuracy: 1.0000 - val_loss: 0.1822 - val_accuracy: 0.9994\n",
      "Epoch 17/25\n",
      "52/52 [==============================] - 34s 647ms/step - loss: 0.1731 - accuracy: 1.0000 - val_loss: 0.1738 - val_accuracy: 0.9976\n",
      "Epoch 18/25\n",
      "52/52 [==============================] - 35s 670ms/step - loss: 0.1612 - accuracy: 1.0000 - val_loss: 0.1616 - val_accuracy: 0.9976\n",
      "Epoch 19/25\n",
      "52/52 [==============================] - 36s 685ms/step - loss: 0.1502 - accuracy: 1.0000 - val_loss: 0.1505 - val_accuracy: 0.9982\n",
      "Epoch 20/25\n",
      "52/52 [==============================] - 38s 723ms/step - loss: 0.1401 - accuracy: 1.0000 - val_loss: 0.1458 - val_accuracy: 0.9958\n",
      "Epoch 21/25\n",
      "52/52 [==============================] - 27s 519ms/step - loss: 0.1308 - accuracy: 1.0000 - val_loss: 0.1630 - val_accuracy: 0.9873\n",
      "Epoch 22/25\n",
      "52/52 [==============================] - 26s 494ms/step - loss: 0.2384 - accuracy: 0.9827 - val_loss: 6876.2759 - val_accuracy: 0.1416\n",
      "Epoch 23/25\n",
      "52/52 [==============================] - 26s 498ms/step - loss: 0.1872 - accuracy: 0.9967 - val_loss: 119.8752 - val_accuracy: 0.1693\n",
      "Epoch 24/25\n",
      "52/52 [==============================] - 26s 495ms/step - loss: 0.1719 - accuracy: 0.9994 - val_loss: 4.2985 - val_accuracy: 0.5753\n",
      "Epoch 25/25\n",
      "52/52 [==============================] - 25s 484ms/step - loss: 0.1614 - accuracy: 0.9998 - val_loss: 0.3478 - val_accuracy: 0.9416\n",
      "CPU times: user 17min 6s, sys: 10min 4s, total: 27min 11s\n",
      "Wall time: 14min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# training the model \n",
    "\n",
    "version2 = model.fit(X_train, y_train, epochs=25, validation_data=(X_test, y_test), callbacks=[mcp_save], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training history\n",
    "#  \"Accuracy\"\n",
    "plt.plot(version2.history['accuracy'])\n",
    "plt.plot(version2.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot(version2.history['loss'])\n",
    "plt.plot(version2.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation']) #loc='lower right'\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 9s 42ms/step - loss: 0.3084 - accuracy: 0.9479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3084007799625397, 0.9478915929794312]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = version2.history['accuracy']\n",
    "np.amax(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   downwards       1.00      1.00      1.00       221\n",
      "       front       1.00      1.00      1.00       467\n",
      "        side       1.00      0.89      0.94       892\n",
      "     upwards       0.45      1.00      0.62        80\n",
      "\n",
      "    accuracy                           0.94      1660\n",
      "   macro avg       0.86      0.97      0.89      1660\n",
      "weighted avg       0.97      0.94      0.95      1660\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "y_test_numeric = y_test.rename(columns={\"downwards\": 0, \"front\": 1, \"side\": 2, \"upwards\": 3})\n",
    "predictions = model.predict(x=X_test, batch_size=128)\n",
    "print(classification_report(y_test_numeric.idxmax(axis=\"columns\").values,\n",
    "                            predictions.argmax(axis=1), target_names=['downwards', 'front', 'side', 'upwards']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating our Video Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Model architecture and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "from glob import glob\n",
    "from scipy import stats as s\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without transferlearning based on:\n",
    "# https://www.pyimagesearch.com/2018/12/31/keras-conv2d-and-convolutional-layers/\n",
    "init=\"he_normal\"\n",
    "inputShape = (480, 640, 3)\n",
    "reg = l2(0.0005)\n",
    "chanDim = -1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (7, 7), strides=(2, 2), padding=\"valid\",\n",
    "                 kernel_initializer=init, kernel_regularizer=reg,input_shape=inputShape))\n",
    "\n",
    "# here we stack two CONV layers on top of each other where\n",
    "# each layerswill learn a total of 32 (3x3) filters\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# stack two more CONV layers, keeping the size of each filter\n",
    "# as 3x3 but increasing to 64 total learned filters\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "# increase the number of filters again, this time to 128\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\",kernel_initializer=init, kernel_regularizer=reg))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization(axis=chanDim))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# fully-connected layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, kernel_initializer=init))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "# softmax classifier\n",
    "model.add(Dense(4))\n",
    "model.add(Activation(\"softmax\"))\n",
    "# return the constructed network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the trained weights\n",
    "model.load_weights(\"weight_v2.1.hdf5\")\n",
    "\n",
    "# compiling the model\n",
    "model.compile(loss='categorical_crossentropy',optimizer='Adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions for all test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in test labels for verification later\n",
    "train = pd.read_csv('material/test_labels.csv', sep=';')\n",
    "\n",
    "# creating the dummy tags\n",
    "trainLabel = pd.read_csv('material/train_frames.csv')\n",
    "y = trainLabel['class']\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:32<00:00,  1.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# creating two lists to store predicted and actual tags\n",
    "predict = []\n",
    "actual = []\n",
    "\n",
    "# for loop to extract frames from each test video\n",
    "for i in tqdm(range(train['NameOfFile'].shape[0])):\n",
    "    count = 0\n",
    "    videoFile = train['NameOfFile'][i]\n",
    "    cap = cv2.VideoCapture('material/raw_test_videos/'+videoFile)   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    \n",
    "    # removing all other files from the temp folder\n",
    "    files = glob('temp/*')\n",
    "    for f in files:\n",
    "        os.remove(f)\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        #if (frameId % math.floor(frameRate) == 0):\n",
    "            # storing the frames of this particular video in temp folder\n",
    "        else:\n",
    "            filename ='temp/' + \"_frame%d.jpg\" % count;count+=1\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()\n",
    "    \n",
    "    # reading all the frames from temp folder\n",
    "    images = glob(\"temp/*.jpg\")\n",
    "    \n",
    "    prediction_images = []\n",
    "    for i in range(len(images)):\n",
    "        img = image.load_img(images[i], target_size=(480, 640, 3))\n",
    "        img = image.img_to_array(img)\n",
    "        img = img/480\n",
    "        prediction_images.append(img)\n",
    "    \n",
    "    # converting all the frames for a test video into numpy array\n",
    "    prediction_images = np.array(prediction_images)\n",
    "\n",
    "    # predicting tags for each array\n",
    "    # predicting tags for each array\n",
    "    prediction = (model.predict(prediction_images) > 0.5).astype(\"int32\")\n",
    "    predictFrame = []\n",
    "    for pre in prediction:\n",
    "        if pre[0] == 1:\n",
    "            predictFrame.append('downwards')\n",
    "        elif pre[1] == 1:\n",
    "            predictFrame.append('front')\n",
    "        elif pre[2] == 1:\n",
    "            predictFrame.append('side')\n",
    "        else:\n",
    "            predictFrame.append('upwards')\n",
    "\n",
    "    # appending the mode of predictions in predict list to assign the tag to the video\n",
    "    predict.append(s.mode(predictFrame)[0][0])\n",
    "    \n",
    "    # appending the actual tag of the video\n",
    "    actual.append(train['label'].loc[train['NameOfFile'] == videoFile].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the accuracy of the predicted tags\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(predict, actual)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['side',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['side',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'downwards',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'downwards',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'front',\n",
       " 'side',\n",
       " 'side']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model by looking at Frame prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in test labels for verification later\n",
    "train = pd.read_csv('material/test_labels.csv', sep=';')\n",
    "\n",
    "# creating the dummy tags\n",
    "trainLabel = pd.read_csv('material/train_frames.csv')\n",
    "y = trainLabel['class']\n",
    "y = pd.get_dummies(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:11<00:00,  2.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# create all frames and csv with labels and frame name\n",
    "\n",
    "# clear temp folder\n",
    "files = glob('temp/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "frameFileNames = []\n",
    "frameFileLabels = []\n",
    "\n",
    "# for loop to extract frames from each test video\n",
    "for i in tqdm(range(train['NameOfFile'].shape[0])):\n",
    "    count = 0\n",
    "    videoFile = train['NameOfFile'][i]\n",
    "    cap = cv2.VideoCapture('material/raw_test_videos/'+videoFile)   # capturing the video from the given path\n",
    "    frameRate = cap.get(5) #frame rate\n",
    "    x=1\n",
    "\n",
    "    while(cap.isOpened()):\n",
    "        frameId = cap.get(1) #current frame number\n",
    "        ret, frame = cap.read()\n",
    "        if (ret != True):\n",
    "            break\n",
    "        else:\n",
    "            filename ='temp/' + videoFile + \"_frame%d.jpg\" % count;count+=1\n",
    "            frameFileNames.append(filename)\n",
    "            frameFileLabels.append(train['label'][i])\n",
    "            cv2.imwrite(filename, frame)\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "# storing the images and their labels in a dataframe\n",
    "test_frames = pd.DataFrame()\n",
    "test_frames['image'] = frameFileNames\n",
    "test_frames['label'] = frameFileLabels\n",
    "\n",
    "# converting the dataframe into csv file \n",
    "test_frames.to_csv('temp/test_frames.csv',header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predicitons on all frames\n",
    "\n",
    "\n",
    "# creating two lists to store predicted and actual tags\n",
    "predictFrame = []\n",
    "actualFrame = []\n",
    "frameName = []\n",
    "\n",
    "# reading all the frames from temp folder\n",
    "images = glob(\"temp/*.jpg\")\n",
    "\n",
    "# reading actual labels from temp folder\n",
    "train = pd.read_csv('temp/test_frames.csv')\n",
    "\n",
    "# open all frames and convert to np array\n",
    "prediction_images = []\n",
    "for i in range(len(images)):\n",
    "    img = image.load_img(images[i], target_size=(480, 640, 3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/480\n",
    "    prediction_images.append(img)\n",
    "    actualFrame.append(train['label'].loc[train['image'] == images[i]].values[0])\n",
    "    frameName.append(images[i].split('/')[1])\n",
    "\n",
    "# converting all the frames for a test video into numpy array\n",
    "prediction_images = np.array(prediction_images)\n",
    "\n",
    "# predicting tags for each array\n",
    "prediction = (model.predict(prediction_images) > 0.5).astype(\"int32\")\n",
    "for pre in prediction:\n",
    "    if pre[0] == 1:\n",
    "        predictFrame.append('downwards')\n",
    "    elif pre[1] == 1:\n",
    "        predictFrame.append('front')\n",
    "    elif pre[2] == 1:\n",
    "        predictFrame.append('side')\n",
    "    else:\n",
    "        predictFrame.append('upwards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 100.00%\n"
     ]
    }
   ],
   "source": [
    "# checking predicted and actual frames labels which dont match\n",
    "count_corrcet = 0\n",
    "count_false = 0\n",
    "for i in range(0, len(predictFrame)):\n",
    "    if predictFrame[i] == actualFrame[i]:\n",
    "        count_corrcet +=1\n",
    "    else:\n",
    "        print('frame ' + str(i),predictFrame[i], actualFrame[i])\n",
    "        #print(frameName[i])\n",
    "        count_false +=1\n",
    "\n",
    "test_acc = (count_corrcet / (count_corrcet + count_false)) *100\n",
    "print('Test Accuracy {:.2f}%'.format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate single Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear temp folder\n",
    "files = glob('temp/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# extract frames from the video and store them\n",
    "count = 0\n",
    "videoFile = '_tigfCJFLZg_00214.mp4' # should be \"downwards\" but is \"front\"\n",
    "#videoFile = '_8Vy3dlHg2w_00001.mp4' # should be \"upwards\" is \"upwards\"\n",
    "#videoFile = '3PLiUG_DuC8_00346.mp4' # should be \"upwards\" but is \"downwards\"\n",
    "#videoFile = '_tigfCJFLZg_00181.mp4' # should be \"side\" is \"side\"\n",
    "# videoFile = 'Video_62.mp4' # should be \"front\" but includes camera switch\n",
    "cap = cv2.VideoCapture('material/single_videos/'+videoFile)   # capturing the video from the given path\n",
    "frameRate = cap.get(5) #frame rate\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    frameId = cap.get(1) #current frame number\n",
    "    ret, frame = cap.read()\n",
    "    if (ret != True):\n",
    "        break\n",
    "    else:\n",
    "        filename ='temp/' + videoFile + \"_frame%d.jpg\" % count;count+=1\n",
    "        cv2.imwrite(filename, frame)\n",
    "cap.release()\n",
    "\n",
    "# creating two lists to store predicted and actual tags\n",
    "predictFrame = []\n",
    "\n",
    "# reading all the frames from temp folder\n",
    "images = glob(\"temp/*.jpg\")\n",
    "\n",
    "# open all frames and convert to np array\n",
    "prediction_images = []\n",
    "for i in range(len(images)):\n",
    "    img = image.load_img(images[i], target_size=(480, 640, 3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = img/480\n",
    "    prediction_images.append(img)\n",
    "\n",
    "# converting all the frames for a test video into numpy array\n",
    "prediction_images = np.array(prediction_images)\n",
    "\n",
    "# predicting tags for each array\n",
    "prediction = (model.predict(prediction_images) > 0.5).astype(\"int32\")\n",
    "for pre in prediction:\n",
    "    if pre[0] == 1:\n",
    "        predictFrame.append('downwards')\n",
    "    elif pre[1] == 1:\n",
    "        predictFrame.append('front')\n",
    "    elif pre[2] == 1:\n",
    "        predictFrame.append('side')\n",
    "    else:\n",
    "        predictFrame.append('upwards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side',\n",
       " 'side']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'side'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most labels say:\n",
    "s.mode(predictFrame)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct:  0 false:  90\n"
     ]
    }
   ],
   "source": [
    "# checking predicted and actual frames labels which dont match\n",
    "counterA = 0\n",
    "counterB = 0\n",
    "for i in range(0, len(predictFrame)):\n",
    "    if predictFrame[i] == 'downwards':\n",
    "        counterA +=1\n",
    "    else:\n",
    "        counterB +=1 \n",
    "print('correct: ', counterA, 'false: ',counterB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
